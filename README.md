# TRW-GRPO: Trust-Region Weighted Group Relative Policy Optimization

> A trust-region weighting view of GRPO for stable RLHF / reasoning RL with LLMs  
> Âü∫‰∫é‚Äú‰ø°‰ªªÂüüÂä†ÊùÉËßÜËßí‚ÄùÁöÑ GRPO ÊîπÈÄ†ÔºåÁî®‰∫éÊõ¥Á®≥ÂÆöÁöÑÂ§ßÊ®°Âûã RLHF / Êé®ÁêÜÂº∫ÂåñÂ≠¶‰π†

---

## üîç Overview / È°πÁõÆÁÆÄ‰ªã

This repository implements **Trust-Region Weighted GRPO (TRW-GRPO)**, a family of GRPO-style algorithms where
the **gradient is explicitly reweighted by a trust-region weight** `w(r, √Ç)` instead of relying on hard PPO-like clipping.

- We **re-interpret GRPO** as a policy-gradient method with an *implicit* importance-ratio‚Äìdependent weight.
- We show how **PPO clipping, DAPO Clip-Higher, TRPA, MRPO** can all be seen as special cases of **trust-region weighting**.
- We provide **drop-in implementations** of:
  - vanilla GRPO,
  - DAPO-style Clip-Higher GRPO, and
  - several TRW-GRPO variants: triangular, logistic, entropy-aware.

> Êú¨‰ªìÂ∫ìÂÆûÁé∞‰∫Ü **TRW-GRPOÔºàTrust-Region Weighted GRPOÔºâ**ÔºåÂ∞Ü GRPO ËßÜ‰∏∫‚ÄúÂ∏¶‰ø°‰ªªÂüüÊùÉÈáçÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶ÊñπÊ≥ï‚ÄùÔºå
> Áî®ÊòæÂºèÁöÑ `w(r, √Ç)` Âèñ‰ª£‰º†ÁªüÁöÑÁ°¨Ââ™ÂàáÔºàpiecewise 0/1 clippingÔºâÔºå‰ªéËÄåÂú®Á®≥ÂÆöÊÄß„ÄÅÊé¢Á¥¢ÊÄßÂíåÊ†∑Êú¨Âà©Áî®Áéá‰πãÈó¥ÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇ

ÂΩìÂâçÁä∂ÊÄÅÔºö  
- ‚úÖ ÁêÜËÆ∫ & ÊñπÊ≥ïÔºàNeurIPS È£éÊ†ºËÆ∫ÊñáËçâÁ®øÔºâ  
- ‚úÖ PyTorch ÂèÇËÄÉÂÆûÁé∞ÔºàÂèØÊèíÂÖ•Áé∞Êúâ GRPO / RLHF pipelineÔºâ  
- üöß ÂÆûÈ™åËÑöÊú¨ & ÂÆåÊï¥ benchmark Â∞ÜÂú®ÂêéÁª≠Ë°•ÂÖÖ  

---

## ‚ú® Key Ideas / Ê†∏ÂøÉÊÄùÊÉ≥

### Gradient-weighting view of GRPO

Standard GRPO gradient can be written as:

\[
\nabla_\theta L_{\text{GRPO}}
= \mathbb{E}_{x,y \sim \pi_{\text{old}}}
\big[ w_{\text{GRPO}}(r, \hat{A})\, r\, \hat{A}\, \nabla_\theta \log \pi_\theta(y \mid x) \big]
\]

where `r = œÄŒ∏ / œÄold` and `√Ç` is group-normalized advantage.  
In vanilla GRPO, `w_GRPO` is an **implicit piecewise 0/1 function** induced by clipping.

> Ê†áÂáÜ GRPO ÁöÑÊ¢ØÂ∫¶ÂèØ‰ª•ÈáçÂÜô‰∏∫‚ÄúÂ∏¶ÊùÉÈáçÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶‚ÄùÔºåÂÖ∂‰∏≠ `w_GRPO(r, √Ç)` Êú¨Ë¥®‰∏äÊòØÁî± clip ‰∫ßÁîüÁöÑ
> **ÂàÜÊÆµ 0/1 ÊùÉÈáçÂáΩÊï∞**ÔºåÂú® `1¬±Œµ` ‰πãÂ§ñÁõ¥Êé•ÊääÊ¢ØÂ∫¶Êà™Êñ≠„ÄÇ

### Why DAPO uses Clip-Higher?

DAPO observes that symmetric clipping `[1‚àíŒµ, 1+Œµ]` causes **entropy collapse**:
- high-probability tokens can still increase a lot,
- low-probability ‚Äúexploration‚Äù tokens are over-constrained ‚Üí exploration dies.

So DAPO **loosens the upper clip** (`Œµ_high > Œµ_low`), effectively giving larger weight to promising low-probability tokens.

> DAPO ÁöÑ Clip-Higher ÂÆûÈôÖ‰∏äÊòØÂú®ÂÅö‚Äú‰∏çÂØπÁß∞‰ø°‰ªªÂüü‚ÄùÔºö  
> - ‰∏ãÁïåÊõ¥‰∏•Ê†ºÔºàÈò≤Ê≠¢Â•Ω token Ë¢´ËøáÂ∫¶ÂâäÂº±Ôºâ  
> - ‰∏äÁïåÊõ¥ÂÆΩÊùæÔºàËÆ©Á®ÄÊúâ‰ΩÜÈ´ò‰ºòÂäøÁöÑ token ÊúâÊú∫‰ºöË¢´ÊîæÂ§ßÔºâÔºå‰ªéËÄåÁºìËß£ÁÜµÂùçÁº©„ÄÇ

### TRW-GRPO: make the weight explicit

TRW-GRPO keeps the same GRPO structure but **replaces hard clipping** with an **explicit trust-region weight**:

\[
L_{\text{TRW}}(\theta) =
\mathbb{E}[\, w(r, \hat{A})\, r\, \hat{A} \,]
\]

We provide several designs of `w(r, √Ç)`:

- **Triangular / piecewise-linear** trust region  
  - `r ‚àà [1‚àíŒµ_low, 1]`: weight from `w0 ‚Üí 1` (Á∫øÊÄß‰∏äÂçá)  
  - `r ‚àà [1, 1+Œµ_high]`: keep `1` or decay to `w2` (Âπ≥Áºì‰∏ãÈôç)  
- **Smooth logistic** weight  
  - `w(r, √Ç) = œÉ(-Œ± |log r|) ¬∑ œÉ(Œ≥ √Ç) + Œ∑`  
- **Entropy-aware asymmetric** weight  
  - Extra boost for **rare but high-advantage** actions.

> ÁÆÄÂçïÊù•ËØ¥Ôºö  
> - ÂéüÊù•ÁöÑ GRPO ÊòØ‚Äú‰ø°‰ªªÂüü = Á°¨ËæπÁïå + 0/1 ÊùÉÈáç‚Äù  
> - TRW-GRPO ÂèòÊàê‚Äú‰ø°‰ªªÂüü = ËøûÁª≠ / ÂèØË∞ÉÁöÑÊùÉÈáçÂáΩÊï∞ w(r, √Ç)‚Äù  
> - Êõ¥Âπ≥Êªë„ÄÅÊõ¥ÂèØÊéß„ÄÅÊõ¥Êñπ‰æøÁªìÂêà DAPO / TRPA / MRPO Ëøô‰∫õÂ∑•‰Ωú„ÄÇ

---

## üß± Repository Structure / ‰ª£Á†ÅÁªìÊûÑ



Installation

```
conda create -n tinygrpo python=3.10
conda activate tinygrpo
pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121
pip3 install vllm==0.6.3 # or you can install 0.5.4, 0.4.2 and 0.3.1
pip3 install ray
# verl
pip install -e .
# conda install -c nvidia cuda-nvcc=12.1
MAX_JOBS=4 pip3 install flash-attn --no-build-isolation
pip install wandb IPython matplotlib
```

test verl
data process
```
python3 examples/data_preprocess/gsm8k.py \
--local_dataset_path /mnt/sdb1/sdb1_xiaojinsong/datasets/openai/gsm8k \
--local_save_dir /mnt/sdb1/sdb1_xiaojinsong/datasets/openai/gsm8k
```

train
```
local_dataset_path=/mnt/sdb1/sdb1_xiaojinsong/datasets
local_model_path=/mnt/sdb1/sdb1_xiaojinsong/llms
HF_USE_FLASH_ATTENTION_2=0
FLASH_ATTENTION_SKIP=1


python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=$local_dataset_path/openai/gsm8k/train.parquet \
    data.val_files=$local_dataset_path/openai/gsm8k/test.parquet \
    data.train_batch_size=256 \
    data.max_prompt_length=128 \
    data.max_response_length=256 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    actor_rollout_ref.model.path=$local_model_path/Qwen/Qwen3-0.6B \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=20 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=5 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=5 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.7 \
    actor_rollout_ref.rollout.n=3 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=5 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True # new \
    actor_rollout_ref.model.attn_implementation=sdpa \
    algorithm.use_kl_in_reward=False \
    trainer.critic_warmup=0 \
    trainer.logger='["console","wandb"]' \
    trainer.project_name='verl_grpo_example_gsm8k' \
    trainer.experiment_name='qwen3_0_5b_function_rm' \
    trainer.n_gpus_per_node=2 \
    trainer.nnodes=1 \
    trainer.save_freq=20 \
    trainer.test_freq=5 \
    trainer.total_epochs=5 $@
```